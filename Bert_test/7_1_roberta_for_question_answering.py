"""
!pip install transformers[torch]==4.38.2
!pip install datasets===2.13.1
!pip install plotly
"""


import torch
from transformers import AutoModelForQuestionAnswering, AutoTokenizer
from scipy.special import softmax
import plotly.express as px
import pandas as pd
import numpy as np

"""## Data Preparation ðŸ“–

Define the context and question that will be used to demonstrate the BERT model's question-answering capabilities.
"""

context = "The giraffe is a large African hoofed mammal belonging to the genus Giraffa. It is the tallest living terrestrial animal and the largest ruminant on Earth. Traditionally, giraffes were thought to be one species, Giraffa camelopardalis, with nine subspecies. Most recently, researchers proposed dividing them into up to eight extant species due to new research into their mitochondrial and nuclear DNA, as well as morphological measurements. Seven other extinct species of Giraffa are known from the fossil record."
question = "How many giraffe species are there?"

"""## Model Setup ðŸ¤–

Load the BERT model and tokenizer designed for question answering tasks.
"""

model_name = "rmihaylov/bert-base-squad-theseus-bg"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForQuestionAnswering.from_pretrained(model_name)

"""## Question Answering

Tokenize the context and the question, and perform inference to get model predictions.
"""

inputs = tokenizer(question, context, return_tensors="pt")
tokenizer.tokenize(context)

# Running the model and getting the start and end scores
with torch.no_grad():
    outputs = model(**inputs)
start_scores, end_scores = softmax(outputs.start_logits)[0], softmax(outputs.end_logits)[0]

"""### Visualization of Token Scores ðŸŽ¯

Plot the scores associated with each token to understand the model's decision-making process.
"""

# Creating a dataframe with the scores and plotting them
scores_df = pd.DataFrame({
    "Token Position": list(range(len(start_scores))) * 2,
    "Score": list(start_scores) + list(end_scores),
    "Score Type": ["Start"] * len(start_scores) + ["End"] * len(end_scores),
})
px.bar(scores_df, x="Token Position", y="Score", color="Score Type", barmode="group", title="Start and End Scores for Tokens")

"""### Extracting the Answer

Identify the tokens that comprise the answer based on the highest start and end scores.
"""

# Getting the answer from the model
start_idx = np.argmax(start_scores)
end_idx = np.argmax(end_scores)
answer_ids = inputs.input_ids[0][start_idx: end_idx + 1]
answer_tokens = tokenizer.convert_ids_to_tokens(answer_ids)
answer = tokenizer.convert_tokens_to_string(answer_tokens)

"""## Advanced Usage: Sliding Windows & Stride for QA ðŸš€

Define a function to automate the question-answering process with new contexts and questions.
"""

# Part 2
# Defining a function to predict the answer to a question given a context
def predict_answer(context, question):
    inputs = tokenizer(question, context, return_tensors="pt", truncation=True, max_length=512)
    with torch.no_grad():
        outputs = model(**inputs)
    start_scores, end_scores = softmax(outputs.start_logits)[0], softmax(outputs.end_logits)[0]
    start_idx = np.argmax(start_scores)
    end_idx = np.argmax(end_scores)
    confidence_score = (start_scores[start_idx] + end_scores[end_idx]) /2
    answer_ids = inputs.input_ids[0][start_idx: end_idx + 1]
    answer_tokens = tokenizer.convert_ids_to_tokens(answer_ids)
    answer = tokenizer.convert_tokens_to_string(answer_tokens)
    if answer != tokenizer.cls_token:
        return answer, confidence_score
    return None, confidence_score

"""
### Evaluating the Function

Test the `predict_answer` function with various questions and contexts."""

# Defining a new context and predicting answers to some questions
context = """
Ð ÐµÐ¿ÑƒÐ±Ð»Ð¸ÐºÐ° Ð‘ÑŠÐ»Ð³Ð°Ñ€Ð¸Ñ Ðµ Ð´ÑŠÑ€Ð¶Ð°Ð²Ð° Ð² Ð®Ð³Ð¾Ð¸Ð·Ñ‚Ð¾Ñ‡Ð½Ð° Ð•Ð²Ñ€Ð¾Ð¿Ð°. 
Ð“Ñ€Ð°Ð½Ð¸Ñ‡Ð¸ Ð½Ð° ÑÐµÐ²ÐµÑ€ Ñ Ð ÑƒÐ¼ÑŠÐ½Ð¸Ñ, Ð½Ð° Ð·Ð°Ð¿Ð°Ð´ â€“ ÑÑŠÑ Ð¡ÑŠÑ€Ð±Ð¸Ñ Ð¸ Ð ÐµÐ¿ÑƒÐ±Ð»Ð¸ÐºÐ° Ð¡ÐµÐ²ÐµÑ€Ð½Ð° ÐœÐ°ÐºÐµÐ´Ð¾Ð½Ð¸Ñ, Ð½Ð° ÑŽÐ³ â€“ Ñ Ð“ÑŠÑ€Ñ†Ð¸Ñ, Ð½Ð° ÑŽÐ³Ð¾Ð¸Ð·Ñ‚Ð¾Ðº â€“ Ñ Ð¢ÑƒÑ€Ñ†Ð¸Ñ Ð¸ Ð½Ð° Ð¸Ð·Ñ‚Ð¾Ðº â€“ Ñ Ð§ÐµÑ€Ð½Ð¾ Ð¼Ð¾Ñ€Ðµ. 
Ð¡ Ð¿Ð»Ð¾Ñ‰ Ð¿Ð¾Ñ‡Ñ‚Ð¸ 111 000 kmÂ² Ð¸ Ð½Ð°ÑÐµÐ»ÐµÐ½Ð¸Ðµ Ð¾ÐºÐ¾Ð»Ð¾ 6 520 000 Ð´ÑƒÑˆÐ¸ (2021) Ñ‚Ñ Ðµ ÑÑŠÐ¾Ñ‚Ð²ÐµÑ‚Ð½Ð¾ Ð½Ð° 11-Ð¾ Ð¸ 16-Ð¾ Ð¼ÑÑÑ‚Ð¾ Ð² Ð•Ð²Ñ€Ð¾Ð¿ÐµÐ¹ÑÐºÐ¸Ñ ÑÑŠÑŽÐ·. 
Ð¡Ð¾Ñ„Ð¸Ñ Ðµ ÑÑ‚Ð¾Ð»Ð¸Ñ†Ð°Ñ‚Ð° Ð¸ Ð½Ð°Ð¹-Ð³Ð¾Ð»ÐµÐ¼Ð¸ÑÑ‚ Ð³Ñ€Ð°Ð´ Ð² ÑÑ‚Ñ€Ð°Ð½Ð°Ñ‚Ð°, ÑÐ»ÐµÐ´Ð²Ð°Ð½Ð° Ð¾Ñ‚ ÐŸÐ»Ð¾Ð²Ð´Ð¸Ð², Ð’Ð°Ñ€Ð½Ð°, Ð‘ÑƒÑ€Ð³Ð°Ñ Ð¸ Ð ÑƒÑÐµ. 
ÐÐ°Ð¹-Ñ€Ð°Ð½Ð½Ð¸Ñ‚Ðµ ÑÐ²Ð¸Ð´ÐµÑ‚ÐµÐ»ÑÑ‚Ð²Ð° Ð·Ð° Ð¿Ñ€Ð¸ÑÑŠÑÑ‚Ð²Ð¸Ðµ Ð½Ð° Ñ…Ð¾Ð¼Ð¾ ÑÐ°Ð¿Ð¸ÐµÐ½Ñ Ð¿Ð¾ Ð·ÐµÐ¼Ð¸Ñ‚Ðµ Ð½Ð° Ð´Ð½ÐµÑˆÐ½Ð° Ð‘ÑŠÐ»Ð³Ð°Ñ€Ð¸Ñ Ð´Ð°Ñ‚Ð¸Ñ€Ð°Ñ‚ Ð¾Ñ‚ Ð¿Ñ€ÐµÐ´Ð¸ Ð¾ÐºÐ¾Ð»Ð¾ 46 Ñ…Ð¸Ð»ÑÐ´Ð¸ Ð³Ð¾Ð´Ð¸Ð½Ð¸, Ð¸Ð»Ð¸ ÐµÐ¿Ð¾Ñ…Ð°Ñ‚Ð° Ð½Ð° Ð¿Ð°Ð»ÐµÐ¾Ð»Ð¸Ñ‚Ð°.[10][11] 
ÐšÑŠÐ¼ Ð¿ÐµÑ‚Ð¾Ñ‚Ð¾ Ñ…Ð¸Ð»ÑÐ´Ð¾Ð»ÐµÑ‚Ð¸Ðµ Ð¿Ñ€ÐµÐ´Ð¸ Ð½.Ðµ. Ð² ÑÐµÐ²ÐµÑ€Ð¾Ð¸Ð·Ñ‚Ð¾Ñ‡Ð½Ð° Ð‘ÑŠÐ»Ð³Ð°Ñ€Ð¸Ñ Ð¿Ñ€Ð¾Ñ†ÑŠÑ„Ñ‚ÑÐ²Ð° ÐºÑƒÐ»Ñ‚ÑƒÑ€Ð°, ÐºÐ¾ÑÑ‚Ð¾ ÑÑŠÐ·Ð´Ð°Ð²Ð° Ð½Ð°Ð¹-Ñ€Ð°Ð½Ð½Ð¸Ñ‚Ðµ Ð·Ð»Ð°Ñ‚Ð½Ð¸ ÑƒÐºÑ€Ð°ÑˆÐµÐ½Ð¸Ñ Ð² Ð•Ð²Ñ€Ð¾Ð¿Ð°. 
ÐžÑ‚ Ð°Ð½Ñ‚Ð¸Ñ‡Ð½Ð¾ÑÑ‚Ñ‚Ð° Ð´Ð¾ Ð¢ÑŠÐ¼Ð½Ð¸Ñ‚Ðµ Ð²ÐµÐºÐ¾Ð²Ðµ Ð¿Ð¾ Ð·ÐµÐ¼Ð¸Ñ‚Ðµ Ð½Ð° Ð´Ð½ÐµÑˆÐ½Ð° Ð‘ÑŠÐ»Ð³Ð°Ñ€Ð¸Ñ ÑÐµ Ñ€Ð°Ð·Ð²Ð¸Ð²Ð°Ñ‚ ÐºÑƒÐ»Ñ‚ÑƒÑ€Ð¸Ñ‚Ðµ Ð½Ð° Ñ‚Ñ€Ð°ÐºÐ¸Ñ‚Ðµ, Ð´Ñ€ÐµÐ²Ð½Ð¸Ñ‚Ðµ Ð³ÑŠÑ€Ñ†Ð¸, ÐºÐµÐ»Ñ‚Ð¸Ñ‚Ðµ, Ð³Ð¾Ñ‚Ð¸Ñ‚Ðµ Ð¸ Ñ€Ð¸Ð¼Ð»ÑÐ½Ð¸Ñ‚Ðµ. 

"""

len(tokenizer.tokenize(context))
predict_answer(context, "What is coffee?")
predict_answer(context, "What are the most common coffee beans?")
predict_answer(context, "How can I make ice coffee?")
predict_answer(context[4000:], "How many people are dependent on coffee for their income?")

# Defining a function to chunk sentences
def chunk_sentences(sentences, chunk_size, stride):
    chunks = []
    num_sentences = len(sentences)
    for i in range(0, num_sentences, chunk_size - stride):
        chunk = sentences[i: i + chunk_size]
        chunks.append(chunk)
    return chunks

sentences = [
    "Sentence 1.",
    "Sentence 2.",
    "Sentence 3.",
    "Sentence 4.",
    "Sentence 5.",
    "Sentence 6.",
    "Sentence 7.",
    "Sentence 8.",
    "Sentence 9.",
    "Sentence 10."
]


sentences = context.split("\n")
chunked_sentences = chunk_sentences(sentences, chunk_size=3, stride=1)


questions = ["ÐšÐ¾Ð¹ Ðµ Ð½Ð°Ð¹-Ð³Ð¾Ð»ÐµÐ¼Ð¸ÑÑ‚ Ð³Ñ€Ð°Ð´?", "ÐšÐ¾Ð¸ ÑÐ° Ð½Ð°Ð¹-Ð³Ð¾Ð»ÐµÐ¼Ð¸Ñ‚Ðµ Ð³Ñ€Ð°Ð´Ð¾Ð²Ðµ?", "ÐšÐ°ÐºÐ²Ð° Ðµ Ð¿Ð»Ð¾Ñ‰Ñ‚Ð° Ð½Ð° Ð‘ÑŠÐ»Ð³Ð°Ñ€Ð¸Ñ?", "Ð¡ ÐºÐ¾Ð¸ Ð´ÑŠÑ€Ð¶Ð°Ð²Ð¸ Ð³Ñ€Ð°Ð½Ð¸Ñ‡Ð¸ Ð‘ÑŠÐ»Ð³Ð°Ñ€Ð¸Ñ?", "ÐšÐ¾Ñ Ðµ ÑÑ‚Ð¾Ð»Ð¸Ñ†Ð°Ñ‚Ð° Ð½Ð° Ð‘ÑŠÐ»Ð³Ð°Ñ€Ð¸Ñ?"]

answers = {}

for chunk in chunked_sentences:
    sub_context = "\n".join(chunk)
    for question in questions:
        answer, score = predict_answer(sub_context, question)

        if answer:
            if question not in answers:
                answers[question] = (answer, score)
            else:
                if score > answers[question][1]:
                    answers[question] = (answer, score)

print(answers)
